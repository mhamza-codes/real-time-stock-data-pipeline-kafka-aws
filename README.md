# ğŸ“ˆ Real-Time Stock Market Data Pipeline using Kafka & AWS â˜ï¸

This project demonstrates an end-to-end real-time data engineering pipeline for streaming and analyzing stock market data using **Apache Kafka** and **AWS services** including EC2, S3, Glue, and Athena.

---

## ğŸ§  Project Overview

- Simulate stock market data from a CSV file using Python.
- Stream data in real time using **Kafka Producer** to a **Kafka server hosted on EC2**.
- Consume data from Kafka, convert to **Pandas DataFrame**, save as CSV, and upload to **Amazon S3**.
- Use **AWS Glue Crawlers** to catalog the data automatically.
- Query the real-time data using **Amazon Athena**.

---

## ğŸ“Œ Architecture

![Architecture Diagram](./Architecture.jpg)  <!-- Replace with actual image path if hosted -->

---

## ğŸ› ï¸ Tech Stack

- **Apache Kafka** â€“ Real-time data streaming
- **Python** â€“ Producer & Consumer scripting
- **Pandas** â€“ DataFrame handling
- **Amazon EC2** â€“ Kafka server hosting
- **Amazon S3** â€“ Scalable storage for CSVs
- **AWS Glue** â€“ Data cataloging
- **Amazon Athena** â€“ Serverless querying
- **s3fs** / **boto3** â€“ Python S3 interaction

---

## âš™ï¸ How It Works

1. ğŸ“Š **Kafka Producer** reads stock data from CSV and sends each record as a JSON message to Kafka.
2. ğŸ›°ï¸ **Kafka Consumer** receives messages, converts them into a DataFrame, and writes to CSV.
3. â˜ï¸ The CSV is uploaded to an S3 bucket.
4. ğŸ”„ **AWS Glue Crawler** scans the S3 data and creates/update the Data Catalog.
5. ğŸ” **Amazon Athena** enables SQL-based querying on the structured data.

---

## ğŸš€ Setup Instructions

> âš ï¸ Replace placeholders like `<your-ip>` or `<bucket-name>` with actual values in the code.

1. Set up a Kafka server on EC2 and allow inbound ports (e.g., 9092).
2. Configure `KafkaProducer.ipynb` to:
   - Load stock CSV
   - Set correct Kafka broker IP
   - Send data to a topic
3. Configure `KafkaConsumer.ipynb` to:
   - Connect to same topic
   - Convert messages to DataFrame
   - Upload to S3 using `s3fs`
4. Create AWS Glue Crawler and connect it to the S3 bucket.
5. Use Athena to query data in SQL.

---

## ğŸ“š Key Learnings

- Real-time data ingestion using Apache Kafka
- Transforming streamed data before cloud storage
- Serverless data cataloging & querying using AWS Glue and Athena
- Orchestrating multiple cloud services in a real-world pipeline

---

## ğŸ“ Related Files

- `KafkaProducer.ipynb` â€“ Simulates and sends data to Kafka
- `KafkaConsumer.ipynb` â€“ Consumes, transforms, and stores data in S3

---

## âœ¨ Author

**Muhammad Hamza**  
[LinkedIn](https://www.linkedin.com/in/-muhammad-hamza/)

---

## ğŸ License

MIT License â€“ feel free to use and adapt!
